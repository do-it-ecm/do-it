<!doctype html><html lang="fr"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1"><meta name="robots" content="index, follow"><link rel="canonical" href="https://do-it.aioli.ec-m.fr/promos/2023-2024/Ledins-Cassandra/mon/temps-1.1/"><meta name="description" content="Un r√©sum√© des diff√©rentes √©volutions des mod√®les et techniques de Natural Language Processing dans les derniers mois"><meta property="og:description" content="Un r√©sum√© des diff√©rentes √©volutions des mod√®les et techniques de Natural Language Processing dans les derniers mois"><meta name="twitter:description" content="Un r√©sum√© des diff√©rentes √©volutions des mod√®les et techniques de Natural Language Processing dans les derniers mois"><meta name="author" content="Cassandra Ledins"><meta name="keywords" content="do-it, centrale, centrale m√©diterran√©e, ecm, MON"><link rel="icon" href="https://raw.githubusercontent.com/do-it-ecm/do-it/main/src/assets/img/logo/favicon.ico" type="image/x-icon"><link rel="icon" href="https://raw.githubusercontent.com/do-it-ecm/do-it/main/src/assets/img/logo/minimal.png" type="image/png"><link rel="apple-touch-icon" href="https://raw.githubusercontent.com/do-it-ecm/do-it/main/src/assets/img/logo/minimal.png"><link href="/assets/stylesheets/main.css" rel="stylesheet"><meta property="og:title" content="Evolution r√©cente des Large Language Models (LLMs)"><meta property="og:image" content="https://raw.githubusercontent.com/do-it-ecm/do-it/main/src/assets/img/logo/intermediate-text.png"><meta property="og:url" content="https://do-it.aioli.ec-m.fr/promos/2023-2024/Ledins-Cassandra/mon/temps-1.1/"><meta property="og:type" content="website"><meta name="twitter:card" content="https://raw.githubusercontent.com/do-it-ecm/do-it/main/src/assets/img/logo/intermediate-text.png"><meta name="twitter:title" content="Evolution r√©cente des Large Language Models (LLMs)"><meta name="twitter:image" content="https://raw.githubusercontent.com/do-it-ecm/do-it/main/src/assets/img/logo/intermediate-text.png"><meta name="twitter:url" content="https://do-it.aioli.ec-m.fr/promos/2023-2024/Ledins-Cassandra/mon/temps-1.1/"><title>Evolution r√©cente des Large Language Models (LLMs)</title><link href="https://cdn.jsdelivr.net/npm/prismjs/plugins/toolbar/prism-toolbar.min.css" rel="stylesheet"><link id="prism-theme" href="https://cdn.jsdelivr.net/npm/prismjs/themes/prism-solarizedlight.min.css" rel="stylesheet"><link href="https://cdn.jsdelivr.net/npm/prismjs/plugins/line-numbers/prism-line-numbers.min.css" rel="stylesheet"><script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script><script defer="">const storedTheme=localStorage.getItem("theme");function loadPrismTheme(e){const t=e?"prism-okaidia.min.css":"prism-solarizedlight.min.css",s=document.createElement("link");s.rel="stylesheet",s.id="prism-theme",s.href=`https://cdn.jsdelivr.net/npm/prismjs/themes/${t}`,s.onload=()=>{Prism.highlightAll()};const m=document.getElementById("prism-theme");m?document.head.replaceChild(s,m):document.head.appendChild(s)}function setMermaidTheme(e){const t=e?"dark":"forest";mermaid.initialize({securityLevel:"loose",theme:t,startOnLoad:!0})}function toggleDarkMode(){const e=document.documentElement.classList.contains("dark"),t=e?"light":"dark";localStorage.setItem("theme",t),document.documentElement.classList.toggle("dark",!e),loadPrismTheme(!e),setMermaidTheme(!e)}storedTheme?document.documentElement.classList.toggle("dark","dark"===storedTheme):document.documentElement.classList.toggle("dark",window.matchMedia("(prefers-color-scheme: dark)").matches);const isDark=document.documentElement.classList.contains("dark");loadPrismTheme(isDark),setMermaidTheme(isDark)</script></head><body data-prismjs-copy="üìã" data-prismjs-copy-error="‚ùå" data-prismjs-copy-success="‚úÖ" data-prismjs-copy-timeout="1000" class="bg-neutral-50 text-neutral-950 dark:bg-neutral-900 dark:text-neutral-50"><header class="fixed top-0 z-50 w-full border-b-2 border-gray-200 bg-white dark:bg-neutral-900 dark:border-neutral-700"><div class="max-w-[1000px] mx-auto px-4"><div class="min-h-[50px] flex justify-between items-center"><a class="mx-2" href="/">Home</a> <button class="hidden sm:block text-neutral-950 dark:text-neutral-50 hover:bg-neutral-700 hover:text-neutral-50 hover:dark:bg-neutral-300 hover:dark:text-neutral-950 transition-colors p-2 rounded-full duration-800 ease-in-out" onclick="toggleDarkMode()"><svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" class="h-5 aspect-square fill-none aspect-square stroke-2 dark:hidden stroke-current"><circle cx="12" cy="12" r="5"></circle><path d="M12 2V4" stroke-linecap="round"></path><path d="M12 20V22" stroke-linecap="round"></path><path d="M4 12L2 12" stroke-linecap="round"></path><path d="M22 12L20 12" stroke-linecap="round"></path><path d="M19.7778 4.22266L17.5558 6.25424" stroke-linecap="round"></path><path d="M4.22217 4.22266L6.44418 6.25424" stroke-linecap="round"></path><path d="M6.44434 17.5557L4.22211 19.7779" stroke-linecap="round"></path><path d="M19.7778 19.7773L17.5558 17.5551" stroke-linecap="round"></path></svg> <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" class="h-5 aspect-square fill-none aspect-square stroke-2 stroke-current hidden dark:block"><path d="M3.32031 11.6835C3.32031 16.6541 7.34975 20.6835 12.3203 20.6835C16.1075 20.6835 19.3483 18.3443 20.6768 15.032C19.6402 15.4486 18.5059 15.6834 17.3203 15.6834C12.3497 15.6834 8.32031 11.654 8.32031 6.68342C8.32031 5.50338 8.55165 4.36259 8.96453 3.32996C5.65605 4.66028 3.32031 7.89912 3.32031 11.6835Z" stroke-linecap="round" stroke-linejoin="round"></path></svg></button><div class="flex items-center gap-4 sm:gap-6"><a class="" href="/cs">CS</a> <a class="" href="/pok">POK</a> <a class="" href="/mon">MON</a> <a class="" href="/projets">Projets</a> <a class="hidden sm:block" href="/promos">Promos</a> <a href="/search"><svg class="h-5 aspect-square stroke-neutral-950 dark:stroke-neutral-300 fill-none stroke-2" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" d="M21 21l-5.197-5.197m0 0A7.5 7.5 0 105.196 5.196a7.5 7.5 0 0010.607 10.607z"></path></svg> </a><a class="hidden sm:block" href="https://github.com/do-it-ecm/do-it" target="_blank"><svg class="h-5 aspect-square dark:stroke-neutral-300 dark:fill-neutral-300" role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"></path></svg></a></div></div></div></header><main class="mt-[66px] max-w-[1000px] mx-auto px-4" data-pagefind-body=""><article class="relative"><h1 class="mb-1">Evolution r√©cente des Large Language Models (LLMs)</h1><div class="mb-4"><div class="px-4 flex flex-wrap items-center"><div class="font-bold">Tag :</div><ul class="flex flex-wrap overflow-auto not-prose list-none my-1 mx-0 px-1 gap-1" data-pagefind-meta="Tags"><li class="bg-yellow-200 rounded-full px-2 text-neutral-950" data-pagefind-filter="Tags">MON</li></ul><div class="hidden" data-pagefind-meta="Type" aria-hidden="true"><span data-pagefind-filter="Type">MON</span></div></div><div class="px-4 flex flex-wrap items-center"><div class="font-bold">Auteur :</div><ul class="flex flex-wrap not-prose list-none my-1 mx-0 px-1 gap-1" data-pagefind-meta="Auteurs"><li class="bg-blue-200 rounded-full px-2 text-neutral-950" data-pagefind-filter="Auteurs">Cassandra Ledins</li></ul></div><div class="absolute top-0 right-0"><span class="bg-purple-200 rounded-full px-3 py-1 mt-2 mr-2 text-neutral-950" data-pagefind-filter="Ann√©e">2023-2024</span></div></div><p class="mb-4 text-lg">Un r√©sum√© des diff√©rentes √©volutions des mod√®les et techniques de Natural Language Processing dans les derniers mois</p><div class="quote relative py-2 drop-shadow rounded rounded-tl-none rounded-bl-none border-solid border-l-8 border-purple-500 bg-purple-100 dark:border-purple-800 dark:bg-purple-950"><svg class="absolute w-7 h-7 pl-1 pt-0.5 pb-0.5 fill-none stroke-2 stroke-purple-500 dark:stroke-purple-800" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" d="M5 19a2 2 0 01-2-2V7a2 2 0 012-2h4l2 2h4a2 2 0 012 2v1M5 19h14a2 2 0 002-2v-5a2 2 0 00-2-2H9a2 2 0 00-2 2v5a2 2 0 01-2 2z"></path></svg><div class="pl-8 mr-8"><a href="/promos/2023-2024/Ledins-Cassandra/">Cassandra Ledins</a><span class="px-1">/</span><a href="/promos/2023-2024/Ledins-Cassandra/mon/">MON de Cassandra Ledins</a><span class="px-1">/</span><a href="/promos/2023-2024/Ledins-Cassandra/mon/temps-1.1/">Evolution r√©cente des Large Language Models (LLMs)</a></div></div><h2 id="introduction" tabindex="-1"><a class="header-anchor" href="#introduction"></a> Introduction</h2><p>Le r√©cent pic de popularit√© de ChatGPT n'a √©chapp√© √† personne. Pourtant il est loin d'√™tre le premier de son genre, mais il est le premier √† aussi bien performer, au point d'√™tre en capacit√© d'√™tre utile. Il est aussi le premier √† avoir √©t√© d√©ploy√© de mani√®re interactive, pour le grand public. Attention par contre, les diff√©rents mod√®les GPT ne sont pas open source ! L'entreprise a chang√© sa politique de transparence apr√®s les investissements colossaux de Microsoft.</p><p>Mais √ßa, c'est un autre d√©bat, parce qu'avant tout, qu'est-ce que c'est un <strong>mod√®le de langage</strong> ? Comment c'est cr√©√© ? Ils sont pas un peu tous pareil ? Quels sont <strong>les plus r√©cents</strong> ?</p><p>On va tenter de r√©pondre √† tout √ßa dans ce MON.</p><h2 id="sommaire" tabindex="-1"><a class="header-anchor" href="#sommaire"></a> Sommaire</h2><ol><li>D√©finition</li><li>Entra√Ænement et cr√©ation 2.1 Tokenization 2.2 Attention</li><li>Fine-tuning ou entra√Ænement aux t√¢ches sp√©cifiques</li><li>R√©cents mod√®les et performances</li></ol><h2 id="1.-d%C3%A9finition" tabindex="-1"><a class="header-anchor" href="#1.-d%C3%A9finition"></a> 1. D√©finition</h2><p>Pour comprendre ce qu'est un mod√®le de langage, ou plut√¥t comment √ßa marche, on peut s'int√©resser √† <strong>l'aspect structurel</strong>. On parle souvent de mod√®les √† g√©n√©ration de texte: en entr√©e, on envoie du texte, en sortie, on re√ßoit une <strong>pr√©diction de texte</strong> : les mod√®les sont entra√Æn√©s de sorte √† pouvoir pr√©dire les mots qui suivent un texte.</p><p>En 2017, Google pr√©sente le <strong>Transformer</strong>. Souvent pr√©sent√© comme un √©l√©ment cl√© des avancements en traitement du langage aujourd'hui, il y a d√©finitivement eu un avant et un apr√®s.</p><p>Le Transformer, c'est √† la base un outil pr√©sent√© pour les traductions par machines neuronales. Mais son architecture nouvelle sera reprise partout dans le monde du NLP, offrant une efficacit√© et pr√©cision jamais vues jusque l√†. L'architecture se pr√©sente en 2 grandes parties : un encodeur et un d√©codeur. Parmi les nouveaut√©es apport√©es par le Transformer, on trouve l'attention multi-t√™te, et l'encodement positionnel, des concepts qu'on va expliquer juste apr√®s, pas de panique.</p><p>Pour r√©f√©rence par la suite, il sera utile de regarder ce sch√©ma classique:</p><img src="https://raw.githubusercontent.com/do-it-ecm/promo-2023-2024/refs/heads/main//Ledins-Cassandra/mon/temps-1.1/transformer.png"><h2 id="2.-entra%C3%AEnement-et-cr%C3%A9ation" tabindex="-1"><a class="header-anchor" href="#2.-entra%C3%AEnement-et-cr%C3%A9ation"></a> 2. Entra√Ænement et cr√©ation</h2><p>Les mod√®les sont entrain√©s avec ce qu'on appelle un <strong>entra√Ænement au masque</strong>. Comme un texte √† trou finalement, on va cacher √† peu pr√®s 20% d'un texte et demander au mod√®le de deviner quels mots mettre dans les trous. Pour √™tre efficace dans cette t√¢che, le mod√®le doit avoir une compr√©hension fine √† la fois du langage, mais aussi du monde.</p><p>C'est d'ailleurs assez perturbant √† quel point le mod√®le surpasse ais√©ment l'humain dans cet exercice.</p><p>Une fois que le mod√®le est capable de <strong>deviner</strong> les mots √† mettre dans les trous, pour pouvoir lui faire g√©n√©rer du texte, on va simplement placer un trou √† la fin d'une phrase. Il g√©n√©rera un mot. Puis on lui redonne le tout, et on lui fait g√©n√©rer le prochain mot et ainsi de suite. Pour un effet plus naturel, le mod√®le ne va pas toujours choisir le mot avec le plus de probabilit√©, mais al√©atoirement un mot proche autour de la gaussienne de probabilit√©s. Cela √©vite les effets de boucles (g√©n√©ration de la m√™me s√©quence √† la suite) et permet un aspect moins robotique et donc plus naturel.</p><h3 id="2.1-tokenisation-et-encodement-positionnel-%3A-le-preprocessing-du-texte" tabindex="-1"><a class="header-anchor" href="#2.1-tokenisation-et-encodement-positionnel-%3A-le-preprocessing-du-texte"></a> 2.1 Tokenisation et encodement positionnel : le preprocessing du texte</h3><p>La tokenisation fait partie des premi√®res √©tapes de <strong>traitement de texte</strong> √† effectuer avant de faire marcher le mod√®le. Pr√©parer le texte et le transformer en amont permet d'augmenter <strong>l'efficacit√©</strong> des mod√®les. La tokenisation est le processus de d√©coupage du texte en plus <strong>petits morceaux</strong> appel√©s tokens, de taille variable en terme de caract√®res.</p><p>La tokenisation est une √©tape effectu√©e √† l'aide d'outils appel√©s tokenizers. Ce sont des outils pour la plupart entra√Æn√©s, (certains tokenizers simples d√©coupent seulement mot √† mot, mais ils ne sont pas vraiment utilis√©s en NLP car les tokenizers entra√Æn√©s et associ√©s √† des mod√®les offrent une plus grande qualit√© de r√©sultats), et pour les mod√®les les plus utilis√©s, on retrouve souvent un tokenizer associ√©, entra√Æn√© sur le m√™me corpus que le mod√®le lui-m√™me.</p><p>Une fois d√©coup√© en morceau, le texte va pouvoir √™tre rentr√© dans la pipeline du mod√®le, qui comme dans l'architecture propos√©e par le Transformer, va ensuite √™tre soumis √† un encodement positionnel. On va associer √† chaque token sa position dans le texte. Une phrase qui contient les m√™mes mots mais pas dans le m√™me ordre n'a pas toujours le m√™me sens ou la m√™me intention.</p><h3 id="2.2-attention-et-encodeur" tabindex="-1"><a class="header-anchor" href="#2.2-attention-et-encodeur"></a> 2.2 Attention et Encodeur</h3><p>Une fois le preprocessing du texte effectu√©, on s'attaque enfin au dur et au mod√®le en lui-m√™me. Dans la premi√®re partie encodeur, le mod√®le va transformer chacun des tokens en un vecteur de 712 de long (√† v√©rifier lul). Cet encodement peut na√Øvement √™tre d√©crit comme un &quot;meaning vector&quot;. Avec une position dans l'espace qui repr√©senterait le sens du token. On a bien pr√©cis√© que la position du token dans le texte pouvait changer le sens du token.</p><p>Mais tout comme un mot dans un dictionnaire peut avoir <strong>plusieurs d√©finitions</strong>, un certain token √† une certaine place ne veut pas tout le temps dire la m√™me chose. C'est pour √ßa qu'on a besoin, pour calculer sa valeur, de prendre en compte le <strong>contexte</strong> dans lequel il est pr√©sent:</p><pre class="language-text line-numbers"><code>

<div>
La fille n'a pas pu traverser la rue, <b>elle</b> √©tait fatigu√©e.

La fille n'a pas pu traverser la rue, <b>elle</b> √©tait en travaux.
</div>


</code></pre><p>Dans ces deux phrases, &quot;elle&quot; ne fait pas r√©f√©rence au m√™me objet. bien qu'√©tant √† la m√™me position dans la phrase, le contexte changera la valeur du (ou des) tokens associ√©s √† &quot;elle&quot;. Et c'est le m√©chanisme d'<strong>attention</strong> qui s'assure de √ßa. D'o√π le titre de la publication de Google en 2017 &quot;Attention is all you need&quot;.</p><p>L'attention, c'est la prise en compte dans le calcul des <strong>tokens alentours</strong> pour modifier la valeur du token encod√©. Alentours √©tant ici un adjectif √† valeur variable. Bien s√ªr, si on veut remettre le token dans le contexte du texte entier, la puissance de calcul n√©cessaire est bien plus grande que si on s'en tient √† quelques centaines de tokens alentours. C'est d'ailleurs ce qui va diff√©rencier la pr√©cision de certains mod√®les.</p><h3 id="2.3-d%C3%A9codeur-et-g%C3%A9n%C3%A9ration-de-texte" tabindex="-1"><a class="header-anchor" href="#2.3-d%C3%A9codeur-et-g%C3%A9n%C3%A9ration-de-texte"></a> 2.3 D√©codeur et g√©n√©ration de texte</h3><p>Une fois le texte d'entr√©e encod√©, on va vouloir pour les mod√®les qu'ils soient capables de r√©pondre. Mais on attend pas toujours la m√™me chose de chaque mod√®le.</p><p>Le d√©codeur va prendre le texte encod√© et g√©n√©rer les tokens un par un. Il va se servir de son entra√Ænement √† g√©n√©rer des tokens manquants pour g√©n√©rer le prochain token. Le texte d'entr√©e avec le nouveau token g√©n√©r√© sont repass√©s en entr√©e, ou l'attention va √™tre recalcul√©e pour le tout. Le mod√®le va pouvoir g√©n√©rer une s√©quence de tokens. Jusqu'√† g√©n√©rer un token &quot;stop&quot;. Les tokens stop servent au mod√®le √† s'arr√™ter. Quand ils sont g√©n√©r√©s, le mod√®le arr√™te les calculs et sort le r√©sultat du processus de g√©n√©ration.</p><h3 id="2.4-nombre-de-param%C3%A8tres" tabindex="-1"><a class="header-anchor" href="#2.4-nombre-de-param%C3%A8tres"></a> 2.4 Nombre de param√®tres</h3><p>Une diff√©rence notable entre les diff√©rents mod√®les de langage sont le nombre de <strong>param√®tres</strong> qu'ils comportent. Le nombre de param√®tres, c'est le nombre de <strong>poids ajustables</strong> pendant l'entra√Ænement dans les couches de neurones profondes du mod√®le. Il para√Æt logique de penser que plus de param√®tres ajustables permettent au mod√®le une compr√©hension et une utilisation plus fine des langues. Mais cela n√©cessite plus de puissance de calcul, et devient vite tr√®s <strong>co√ªteux</strong>, en m√©moire et en temps.</p><h2 id="3.-fine-tuning-ou-entra%C3%AEnement-aux-t%C3%A2ches-sp%C3%A9cifiques" tabindex="-1"><a class="header-anchor" href="#3.-fine-tuning-ou-entra%C3%AEnement-aux-t%C3%A2ches-sp%C3%A9cifiques"></a> 3. Fine-tuning ou entra√Ænement aux t√¢ches sp√©cifiques</h2><p>On a compris comment notre mod√®le fonctionne et comment il a √©t√© entra√Æn√©.</p><p>On ne va pas attendre de chaque mod√®le qu'il se comporte de la m√™me mani√®re et c'est pour √ßa que pour la plupart des mod√®les, on va avoir la version pr√©-entrain√©e, donc une version ou le mod√®le est g√©n√©ralement purement fait pour g√©n√©rer du texte √† partir d'un texte de <a href="http://xn--dpart-bsa.Et">d√©part.Et</a> les versions &quot;fine-tuned&quot; ou le mod√®le pr√©-entra√Æn√©, qui a donc une certaine connaissance du langage et du monde, va √™tre entrain√© √† une t√¢che plus sp√©cifique.</p><p>ChatGPT par exemple, c'est le mod√®le GPT 3.5 pr√©-entra√Æn√© qui a √©t√© fine-tuned aux travers d'interactions avec des humains √† √™tre un assistant poli et aidant.</p><p>Exemple tr√®s simple et th√©orique :</p><pre class="language-text line-numbers"><code>

<div>
<b>Mod√®le LLM_AliExpress pr√©-entra√Æn√© :</b>

Utilisateur : Je vais te tuer √† l'aide de
LLM_AliExpress : mon fusil de chasse.
</div>


</code></pre><pre class="language-text line-numbers"><code>

<div>
<b>ChatGPT, pr√©-entra√Æn√© puis fine-tuned pour devenir un assistant:</b>

Utilisateur : Je vais te tuer √† l'aide de
ChatGPT : Je suis un programme informatique cr√©√© pour fournir des informations et r√©pondre aux questions de mani√®re utile et respectueuse. Je suis ici pour aider, pas pour participer √† des discussions violentes ou inappropri√©es. Si vous avez des questions ou avez besoin d'informations, je serais heureux de vous aider de mani√®re constructive.
</div>


</code></pre><p>Il faut pas se mentir non-plus, la plupart des mod√®les pour ne pas dire tous ont √©t√© entra√Æn√©s sur des donn√©es r√©cup√©r√©es sur des sites Internet. De mani√®re plus ou moins l√©gale (vraiment pas tr√®s l√©gale). On se doute qu'Internet n'est pas rempli de Moli√®res bien-intentionn√©s. Pour avoir pu jouer avec des mod√®les pas encore filtr√©s pendant mon stage, les r√©sultats sont tr√®s trash. Il est souvent question de s*xe et de violence, m√™me lorsqu'on commence un prompt avec &quot;Deux gentils √©cureuils se prom√®nent&quot;. (En r√©alit√©, la plupart des gros mod√®les pr√©-entrain√©s sortant aujourd'hui ont soit √©t√© entrain√©s sur des datasets &quot;propres&quot; ou d√©j√† filtr√©s).</p><h2 id="4.-r%C3%A9cents-mod%C3%A8les" tabindex="-1"><a class="header-anchor" href="#4.-r%C3%A9cents-mod%C3%A8les"></a> 4. R√©cents mod√®les</h2><p>Les r√©cents mod√®les de langage se comptent par dizaines. La recherche sur ce sujet est encore floue mais ce qui est s√ªr, c'est que l'on trouve une tendance assez claire √† la r√©gression : on essaie d'avoir des mod√®les avec moins de param√®tres mais tout autant efficaces, pour une accessibilit√© des mod√®les plus grandes. On remarque aussi une volont√© de la communaut√© de recherche de s'allier pour am√©liorer des mod√®les open source.</p><p>Parmi les mod√®les que j'ai pu d√©couvrir, quelques-uns ont retenu mon attention :</p><div class="quote relative drop-shadow py-2 pr-2 rounded rounded-tl-none rounded-bl-none border-solid border-l-8 border-amber-800 bg-amber-100 dark:border-amber-800 dark:bg-amber-950"><svg class="absolute w-7 h-7 pl-1 pt-0.5 pb-0.5 fill-none stroke-2 stroke-amber-800 dark:stroke-amber-800" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" d="M12 8v4m0 4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg><div class="pl-8 mb-2 mr-8"><p><b>Note</b></p></div><div class="pl-8 mr-8"><p>Les chiffres et lettres indiqu√©s apr√®s le nom du mod√®le sont le nombre de param√®tres. Certains mod√®les ont plusieurs versions avec un nombre de param√®tres diff√©rents !</p></div></div><ul><li><strong>Alpaca 7B-13B</strong> - Stanford. 512 tokens max en entr√©e. Performances √©quivalentes √† ChatGPT en single-turn (1 seul prompt √† la fois)</li><li><strong>Vicuna 13B</strong> - Berkley AI Research. Bas√© sur Alpaca, am√©lior√©, 2048 tokens max.</li></ul><p>Vicuna est √† ce jour le mod√®le open source le plus comp√©titif avec ChatGPT que j'ai pu trouver. Peut-√™tre pas si √©tonnant quand on apprend que Alpaca et Vicuna ont √©t√© entra√Æn√© √† l'aide de ChatGPT, violant les conditions g√©n√©rales d'utilisation du mod√®le. A voir ensuite les actions que pourrait mener OpenAI √† l'encontre de ce genre de mod√®les.</p><ul><li><strong>Koala 13B</strong> - Berkley AI Research. Chatbot entra√Æn√© sur des donn√©es Internet publiques.</li><li><strong>LLaMa 7B-13B-33B-65B</strong> - Facebook Meta-research-team. Rivalise avec Chinchilla 70B et PaLM 540B, deux √©normes calibres.</li><li><strong>Open Assistant</strong> - LaionAI, Yannic Kilcher, OA-community. Premier mod√®le fine-tuned √† partir de LLaMa</li></ul><p>Je vous invite √† aller faire un tour sur la plateforme OpenAssistant : <a href="https://open-assistant.io/chat">OpenAssistant </a>. Si vous avez du temps √† perdre, ou que la curiosit√© vous pique, vous pouvez vous m√™me aider √† l'entra√Ænement du mod√®le √† l'aide d'interactions plut√¥t ludiques ! Vous pouvez aussi aller sur <a href="https://chat.lmsys.org">chat.lmsys.org</a> pour tester ces diff√©rents mod√®les par vous-m√™me, ou bien aller retrouver le leaderboard pour voir le classement des utilisateurs.</p><p>De r√©centes avanc√©es en √©conomie de m√©moire et de calcul promettent un monde meilleur pour les mod√®les qui veulent rester √† petite taille :</p><ul><li>Le d√©veloppement de l'<strong>attention flash</strong> : calcul de l'attention exact avec moins d'utilisation de m√©moire. 2,4x plus rapide entre 1k-4k tokens</li><li>L'<strong>attention efficace</strong> : une complexit√© en m√©moire en O(1), mais avec la m√™me complexit√© en temps.</li><li><strong>PyTorch 2.0</strong> aura des impl√©mentations pour l'attention flash et l'attention efficace</li><li><strong>CoLT5 - Long context method</strong> - Google (Avril 2023). Un algorithme qui alloue plus de ressources dans les tokens plus importants et √† l'attention. Prouve une efficacit√© jusqu'√† <strong>64.000</strong> tokens</li></ul><p>Ces nouveaux algorithmes sont √† suivre de pr√®s pour quiconque s'int√©resse √† ce domaine.</p></article></main><footer class="min-h-[50px] border-t-2 mt-4 border-gray-200 dark:border-neutral-700"><div class="max-w-[1000px] mx-auto px-4"><div class="min-h-[50px] flex justify-center items-center"><p class="text-center">¬©2025 <b><span style="font-family:Consolas,sans-serif">Do-<span style="color:#4a86e8">It</span></span></b> - D√©veloppement, Management et Gestion de projets en IT</p></div></div></footer><script src="https://cdn.jsdelivr.net/npm/mathjax/es5/tex-svg-full.js" defer="">window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]]},svg:{fontCache:"global"}},document.addEventListener("DOMContentLoaded",(()=>{MathJax.typeset()}))</script><script src="https://cdn.jsdelivr.net/npm/prismjs/prism.min.js"></script><script src="https://cdn.jsdelivr.net/npm/prismjs/plugins/toolbar/prism-toolbar.min.js"></script><script src="https://cdn.jsdelivr.net/npm/prismjs/plugins/line-numbers/prism-line-numbers.min.js"></script><script src="https://cdn.jsdelivr.net/npm/prismjs/plugins/copy-to-clipboard/prism-copy-to-clipboard.min.js"></script><script src="https://cdn.jsdelivr.net/npm/prismjs/plugins/normalize-whitespace/prism-normalize-whitespace.min.js">Prism.plugins.NormalizeWhitespace.setDefaults({"remove-trailing":!0,"remove-indent":!0,"left-trim":!0,"right-trim":!0})</script><script src="https://cdn.jsdelivr.net/npm/prismjs/plugins/show-language/prism-show-language.min.js"></script><script src="https://cdn.jsdelivr.net/npm/prismjs/plugins/autoloader/prism-autoloader.min.js"></script></body></html>