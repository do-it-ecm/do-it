<!doctype html><html lang="fr"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1"><meta name="robots" content="index, follow"><link rel="canonical" href="https://do-it.aioli.ec-m.fr/promos/2024-2025/Gonin-Emma/mon/temps-3.1/"><meta name="description" content="Un MON traitant de l&#39;√©thique de l&#39;intelligence artificielle dans le secteur de la sant√©."><meta property="og:description" content="Un MON traitant de l&#39;√©thique de l&#39;intelligence artificielle dans le secteur de la sant√©."><meta name="twitter:description" content="Un MON traitant de l&#39;√©thique de l&#39;intelligence artificielle dans le secteur de la sant√©."><meta name="author" content="Emma Gonin"><meta name="keywords" content="do-it, centrale, centrale m√©diterran√©e, ecm, MON, temps 3, vert, bleu, d√©butant"><link rel="icon" href="https://raw.githubusercontent.com/do-it-ecm/do-it/main/src/assets/img/logo/favicon.ico" type="image/x-icon"><link rel="icon" href="https://raw.githubusercontent.com/do-it-ecm/do-it/main/src/assets/img/logo/minimal.png" type="image/png"><link rel="apple-touch-icon" href="https://raw.githubusercontent.com/do-it-ecm/do-it/main/src/assets/img/logo/minimal.png"><link href="/assets/stylesheets/main.css" rel="stylesheet"><meta property="og:title" content="MON 3.1 - Ethique de l&#39;intelligence artificielle - Explainable AI"><meta property="og:image" content="https://raw.githubusercontent.com/do-it-ecm/do-it/main/src/assets/img/logo/intermediate-text.png"><meta property="og:url" content="https://do-it.aioli.ec-m.fr/promos/2024-2025/Gonin-Emma/mon/temps-3.1/"><meta property="og:type" content="website"><meta name="twitter:card" content="https://raw.githubusercontent.com/do-it-ecm/do-it/main/src/assets/img/logo/intermediate-text.png"><meta name="twitter:title" content="MON 3.1 - Ethique de l&#39;intelligence artificielle - Explainable AI"><meta name="twitter:image" content="https://raw.githubusercontent.com/do-it-ecm/do-it/main/src/assets/img/logo/intermediate-text.png"><meta name="twitter:url" content="https://do-it.aioli.ec-m.fr/promos/2024-2025/Gonin-Emma/mon/temps-3.1/"><title>MON 3.1 - Ethique de l&#39;intelligence artificielle - Explainable AI</title><link href="https://cdn.jsdelivr.net/npm/prismjs/plugins/toolbar/prism-toolbar.min.css" rel="stylesheet"><link id="prism-theme" href="https://cdn.jsdelivr.net/npm/prismjs/themes/prism-solarizedlight.min.css" rel="stylesheet"><link href="https://cdn.jsdelivr.net/npm/prismjs/plugins/line-numbers/prism-line-numbers.min.css" rel="stylesheet"><script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script><script defer="">const storedTheme=localStorage.getItem("theme");function loadPrismTheme(e){const t=e?"prism-okaidia.min.css":"prism-solarizedlight.min.css",s=document.createElement("link");s.rel="stylesheet",s.id="prism-theme",s.href=`https://cdn.jsdelivr.net/npm/prismjs/themes/${t}`,s.onload=()=>{Prism.highlightAll()};const m=document.getElementById("prism-theme");m?document.head.replaceChild(s,m):document.head.appendChild(s)}function setMermaidTheme(e){const t=e?"dark":"forest";mermaid.initialize({securityLevel:"loose",theme:t,startOnLoad:!0})}function toggleDarkMode(){const e=document.documentElement.classList.contains("dark"),t=e?"light":"dark";localStorage.setItem("theme",t),document.documentElement.classList.toggle("dark",!e),loadPrismTheme(!e),setMermaidTheme(!e)}storedTheme?document.documentElement.classList.toggle("dark","dark"===storedTheme):document.documentElement.classList.toggle("dark",window.matchMedia("(prefers-color-scheme: dark)").matches);const isDark=document.documentElement.classList.contains("dark");loadPrismTheme(isDark),setMermaidTheme(isDark)</script></head><body data-prismjs-copy="üìã" data-prismjs-copy-error="‚ùå" data-prismjs-copy-success="‚úÖ" data-prismjs-copy-timeout="1000" class="bg-neutral-50 text-neutral-950 dark:bg-neutral-900 dark:text-neutral-50"><header class="fixed top-0 z-50 w-full border-b-2 border-gray-200 bg-white dark:bg-neutral-900 dark:border-neutral-700"><div class="max-w-[1000px] mx-auto px-4"><div class="min-h-[50px] flex justify-between items-center"><a class="mx-2" href="/">Home</a> <button class="hidden sm:block text-neutral-950 dark:text-neutral-50 hover:bg-neutral-700 hover:text-neutral-50 hover:dark:bg-neutral-300 hover:dark:text-neutral-950 transition-colors p-2 rounded-full duration-800 ease-in-out" onclick="toggleDarkMode()"><svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" class="h-5 aspect-square fill-none aspect-square stroke-2 dark:hidden stroke-current"><circle cx="12" cy="12" r="5"></circle><path d="M12 2V4" stroke-linecap="round"></path><path d="M12 20V22" stroke-linecap="round"></path><path d="M4 12L2 12" stroke-linecap="round"></path><path d="M22 12L20 12" stroke-linecap="round"></path><path d="M19.7778 4.22266L17.5558 6.25424" stroke-linecap="round"></path><path d="M4.22217 4.22266L6.44418 6.25424" stroke-linecap="round"></path><path d="M6.44434 17.5557L4.22211 19.7779" stroke-linecap="round"></path><path d="M19.7778 19.7773L17.5558 17.5551" stroke-linecap="round"></path></svg> <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" class="h-5 aspect-square fill-none aspect-square stroke-2 stroke-current hidden dark:block"><path d="M3.32031 11.6835C3.32031 16.6541 7.34975 20.6835 12.3203 20.6835C16.1075 20.6835 19.3483 18.3443 20.6768 15.032C19.6402 15.4486 18.5059 15.6834 17.3203 15.6834C12.3497 15.6834 8.32031 11.654 8.32031 6.68342C8.32031 5.50338 8.55165 4.36259 8.96453 3.32996C5.65605 4.66028 3.32031 7.89912 3.32031 11.6835Z" stroke-linecap="round" stroke-linejoin="round"></path></svg></button><div class="flex items-center gap-4 sm:gap-6"><a class="" href="/cs">CS</a> <a class="" href="/pok">POK</a> <a class="" href="/mon">MON</a> <a class="" href="/projets">Projets</a> <a class="hidden sm:block" href="/promos">Promos</a> <a href="/search"><svg class="h-5 aspect-square stroke-neutral-950 dark:stroke-neutral-300 fill-none stroke-2" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" d="M21 21l-5.197-5.197m0 0A7.5 7.5 0 105.196 5.196a7.5 7.5 0 0010.607 10.607z"></path></svg> </a><a class="hidden sm:block" href="https://github.com/do-it-ecm/do-it" target="_blank"><svg class="h-5 aspect-square dark:stroke-neutral-300 dark:fill-neutral-300" role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"></path></svg></a></div></div></div></header><main class="mt-[66px] max-w-[1000px] mx-auto px-4" data-pagefind-body=""><article class="relative"><h1 class="mb-1">MON 3.1 - Ethique de l&#39;intelligence artificielle - Explainable AI</h1><div class="mb-4"><div class="px-4 flex flex-wrap items-center"><div class="font-bold">Tags :</div><ul class="flex flex-wrap overflow-auto not-prose list-none my-1 mx-0 px-1 gap-1" data-pagefind-meta="Tags"><li class="bg-yellow-200 rounded-full px-2 text-neutral-950" data-pagefind-filter="Tags">MON</li><li class="bg-yellow-200 rounded-full px-2 text-neutral-950" data-pagefind-filter="Tags">temps 3</li><li class="bg-yellow-200 rounded-full px-2 text-neutral-950" data-pagefind-filter="Tags">vert</li><li class="bg-yellow-200 rounded-full px-2 text-neutral-950" data-pagefind-filter="Tags">bleu</li><li class="bg-yellow-200 rounded-full px-2 text-neutral-950" data-pagefind-filter="Tags">d√©butant</li></ul><div class="hidden" data-pagefind-meta="Type" aria-hidden="true"><span data-pagefind-filter="Type">MON</span></div></div><div class="px-4 flex flex-wrap items-center"><div class="font-bold">Auteur :</div><ul class="flex flex-wrap not-prose list-none my-1 mx-0 px-1 gap-1" data-pagefind-meta="Auteurs"><li class="bg-blue-200 rounded-full px-2 text-neutral-950" data-pagefind-filter="Auteurs">Emma Gonin</li></ul></div><div class="absolute top-0 right-0"><span class="bg-purple-200 rounded-full px-3 py-1 mt-2 mr-2 text-neutral-950" data-pagefind-filter="Ann√©e">2024-2025</span></div></div><p class="mb-4 text-lg">Un MON traitant de l'√©thique de l'intelligence artificielle dans le secteur de la sant√©.</p><div class="quote relative py-2 drop-shadow rounded rounded-tl-none rounded-bl-none border-solid border-l-8 border-purple-500 bg-purple-100 dark:border-purple-800 dark:bg-purple-950"><svg class="absolute w-7 h-7 pl-1 pt-0.5 pb-0.5 fill-none stroke-2 stroke-purple-500 dark:stroke-purple-800" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" d="M5 19a2 2 0 01-2-2V7a2 2 0 012-2h4l2 2h4a2 2 0 012 2v1M5 19h14a2 2 0 002-2v-5a2 2 0 00-2-2H9a2 2 0 00-2 2v5a2 2 0 01-2 2z"></path></svg><div class="pl-8 mr-8"><a href="/promos/2024-2025/Gonin-Emma/">Emma Gonin</a><span class="px-1">/</span><a href="/promos/2024-2025/Gonin-Emma/mon/">MON de Emma Gonin</a><span class="px-1">/</span><a href="/promos/2024-2025/Gonin-Emma/mon/temps-3.1/">MON 3.1 - Ethique de l&#39;intelligence artificielle - Explainable AI</a></div></div><div class="quote relative py-2 drop-shadow rounded rounded-tl-none rounded-bl-none border-solid border-l-8 border-pink-500 bg-pink-100 dark:border-pink-800 dark:bg-pink-950"><svg class="absolute w-7 h-7 pl-1 pt-0.5 pb-0.5 fill-none stroke-2 stroke-pink-500 dark:stroke-pink-800" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.253v13m0-13C10.832 5.477 9.246 5 7.5 5S4.168 5.477 3 6.253v13C4.168 18.477 5.754 18 7.5 18s3.332.477 4.5 1.253m0-13C13.168 5.477 14.754 5 16.5 5c1.747 0 3.332.477 4.5 1.253v13C19.832 18.477 18.247 18 16.5 18c-1.746 0-3.332.477-4.5 1.253"></path></svg><div class="pl-8 mb-2 mr-8"><p><b>Pr√©requis</b></p></div><div class="pl-8 mr-8"><p>Aucun</p></div></div><div class="quote relative py-2 drop-shadow rounded rounded-tl-none rounded-bl-none border-solid border-l-8 border-emerald-500 bg-emerald-100 dark:border-emerald-800 dark:bg-emerald-950"><svg class="absolute w-6 h-6 pl-1 pt-0.5 pb-0.5 fill-none stroke-2 stroke-emerald-500 dark:stroke-emerald-800" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" d="M13.19 8.688a4.5 4.5 0 011.242 7.244l-4.5 4.5a4.5 4.5 0 01-6.364-6.364l1.757-1.757m13.35-.622l1.757-1.757a4.5 4.5 0 00-6.364-6.364l-4.5 4.5a4.5 4.5 0 001.242 7.244"></path></svg><div class="pl-8 mb-2 mr-8"><p><b>Liens</b></p></div><div class="pl-8 mr-8"><ul><li>[1] <a href="https://www.hub-franceia.fr/wp-content/uploads/2024/12/Guide_pratique_IA_ethique_version-2.pdf">Guide pratique IA √©thique V2 France Hub IA</a></li><li>[2] Un article super int√©ressant sur comment l'IA explicable peut b√©n√©ficier aux d√©veloppeurs, aux cliniciens, aux patients du point de vue √©thique et l√©gal : <a href="https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-01332-6">Explainability for artificial intelligence in healthcare: a multidisciplinary perspective</a></li><li>[3] <a href="https://www.sciencedirect.com/science/article/pii/S0045790624002982">Zahra Sadeghi et al., A review of Explainable Artificial Intelligence in healthcare</a></li><li>[4] <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC9862413/">Survey of Explainable AI Techniques in Healthcare</a></li><li>[5] <a href="https://arxiv.org/pdf/1711.11279">Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV), Been Kim et al.</a></li></ul></div></div><p>Je vais faire mon TFE sur un sujet combinant intelligence artificielle et d√©veloppement web dans le secteur de la sant√©. Apr√®s les recommandations de Laetitia Piet, j'ai lu le <a href="https://www.hub-franceia.fr/wp-content/uploads/2024/12/Guide_pratique_IA_ethique_version-2.pdf">livre blanc</a> de France Hub IA sur lequel elle a travaill√©, qui est un guide pratique sur l'√©thique de l'Intelligence artificielle. [1]</p><p>Ce livre blanc √©nonce les 7 points cl√©s de l'IA <strong>√©thique</strong> :</p><ul><li>S√ªret√©</li><li>Impact durable</li><li>Autonomie</li><li>Responsabilit√© humaine</li><li>Explicabilit√©</li><li>Equit√©</li><li>Respect de la vie priv√©e</li></ul><p>Il y a de multiples utilisations de l'IA dans le domaine de la sant√©, pour n'en citer que quelques unes, la segmentation d'images m√©dicales pour d√©tecter une ou des tumeurs (mon sujet de stage), le suivi des signes vitaux d'un patient avec des dispositifs port√©s par le patient, l'automatisation de t√¢ches r√©p√©titives effectu√©es par les m√©decins (cr√©ation de rapports m√©dicaux)... Des donn√©es sensibles sont utilis√©es pour entra√Æner ces IA, et la question de l'√©thique est d'autant plus essentielle quant au traitement de ces donn√©es et des r√©sultats fournis par les IA. Apr√®s avoir lu le livre blanc, j'ai d√©cid√© d'aller plus loin pour un des points √©nonc√©s ci-dessus : l'explicabilit√©. En effet, je me suis pos√©e une question : <strong>Comment les praticiens (m√©decins...) et les patients, qui sont les utilisateurs finaux de l'IA, peuvent non seulement comprendre les r√©sultats fournis par l'IA mais aussi croire √† leur fiabilit√© ?</strong> Qu'est-ce qui prouve qu'une tumeur d√©tect√©e par une IA sur une image m√©dicale est vraiment une tumeur ? Quel f√ªt le raisonnement de l'IA pour affirmer que ce qui est difficilement d√©tectable par l'homme humain, est bien une tumeur ? D'ailleurs, √† quel niveau d'incertitude affirme-t'elle l'existence de cette tumeur ? Nous allons voir dans ce MON quelques pistes pour d√©cortiquer plus facilement la <strong>&quot;bo√Æte noire&quot;</strong> (ou Black box) qu'est l'IA pour permettre aux utilisateurs finaux de comprendre.</p><h2 id="introduction" tabindex="-1"><a class="header-anchor" href="#introduction"></a> Introduction</h2><p>On peut distinguer deux types de mod√®les d'IA : ceux qui sont dit <em>&quot;interpr√©tables&quot;</em>, appel√©s √©galement &quot;white-box&quot; et les autres qui sont dit <em>&quot;non-interpr√©tables&quot;</em>. Ce qui rend un algorithme interpr√©table est la <strong>clart√© de la relation entre les donn√©es d'entr√©e et les donn√©es de sortie</strong>. Par exemple, les mod√®les de r√©gression lin√©aire et les arbres de d√©cision font partis des mod√®les interpr√©tables : ils ont des caract√©ristiques claires qui expliquent l'influence des variables sur les r√©sultats fournis par l'IA. En revanche, les mod√®les non-interpr√©tables sont des structures complexes avec des poids, des param√®tres qui sont inconnus ! Les algorithmes de Deep Learning font partis de ces &quot;black-box models&quot;. Ils sont √©galement sujets √† des <em>biais</em> qui faussent les r√©sultats et qui sont probl√©matiques aux yeux des cliniciens.</p><p>Les recherches sur l'intelligence artificielle explicable ont explos√© ces derni√®res ann√©es !</p><h2 id="challenges-et-limites-de-l'ia-dite-%22opaque%22" tabindex="-1"><a class="header-anchor" href="#challenges-et-limites-de-l'ia-dite-%22opaque%22"></a> Challenges et limites de l'IA dite &quot;opaque&quot;</h2><p>L'explicabilit√© des mod√®les d'IA est essentielle pour s'assurer que leur performance repose sur des donn√©es pertinentes et non sur des facteurs externes, comme des m√©tadonn√©es. Un exemple c√©l√®bre est celui d'un mod√®le qui classait des huskies et des loups en se basant sur la neige en arri√®re-plan, plut√¥t que sur les animaux eux-m√™mes. En m√©decine, un mod√®le d'IA utilis√© pour d√©tecter des patients √† haut risque √† partir de radiographies a √©chou√© lorsqu'il a √©t√© test√© en dehors de son h√¥pital d'origine, car il avait appris √† identifier le type de machine utilis√©e plut√¥t que des informations cliniques pertinentes ! [2]</p><p>Il est important de mentionner les <em>biais</em> dans l'IA : l‚Äôing√©nieur que j'ai interview√© souligne que l'IA peut √™tre biais√©e si elle est entra√Æn√©e sur des donn√©es limit√©es ou homog√®nes (par exemple, provenant d'un seul h√¥pital). Ce type de biais pourrait affecter la capacit√© du mod√®le √† g√©n√©raliser √† des cas r√©els et vari√©s. La <strong>transparence</strong> d'une IA explicable permet de d√©tecter en amont et comprendre ces biais, ce qui aide √† ajuster les mod√®les pour √©viter des erreurs cliniques.</p><h2 id="transparence%2C-justifiabilit%C3%A9-et-intelligibilit%C3%A9-%3A-les-ma%C3%AEtres-mots-de-l'intelligence-artificielle-explicable" tabindex="-1"><a class="header-anchor" href="#transparence%2C-justifiabilit%C3%A9-et-intelligibilit%C3%A9-%3A-les-ma%C3%AEtres-mots-de-l'intelligence-artificielle-explicable"></a> Transparence, justifiabilit√© et intelligibilit√© : les ma√Ætres-mots de l'intelligence artificielle explicable</h2><p><img src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/ee9e/9862413/b7e22e073325/sensors-23-00634-g001.jpg" alt="int√©r√™t de l'xai"></p><p>Cette figure provenant de <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC9862413/">cet article</a> repr√©sente l'int√©r√™t de l'IA explicable sur la d√©tection d'une infection de Covid-19.</p><ul><li>Le premier mod√®le est une <em>black box</em>, l'utilisateur a juste en sortie le r√©sultat &quot;infect√©&quot; mais ne sait pas le pourquoi du comment. Le mod√®le est opaque et la confiance de l'utilisateur envers ce mod√®le chute.</li><li>Le second output est une <em>carte de chaleur</em> qui montre les parties qui sont probablement infect√©es : l'utilisateur comprend et v√©rifie que les parties d√©tect√©es par l'IA sont bien infect√©es, il sait o√π aller chercher l'information.</li><li>Le troisi√®me output est une <em>comparaison entre la donn√©e d'entr√©e (input) et une donn√©e labelis√©e</em> avec laquelle l'IA a compar√© la donn√©e d'entr√©e. L'utilisateur alors peut avoir une confiance plus grande dans le r√©sultat de l'IA.</li></ul><p>Les cliniciens ne sont pas toujours int√©ress√©s par le fonctionnement technique de l'IA, mais veulent s'assurer que les r√©sultats sont <strong>fiables</strong>. L'IA explicable permet aux cliniciens de comprendre sur quels <em>crit√®res</em> le mod√®le se base pour prendre des d√©cisions, renfor√ßant ainsi leur confiance dans l'outil. En effet, l'explicabilit√© peut aider les cliniciens √† <strong>√©valuer les recommandations d'un syst√®me</strong> en les comparant √† leur propre exp√©rience et jugement clinique : cela leur permet de d√©cider s'ils doivent ou non se fier aux recommandations du syst√®me et renforce ainsi leur confiance en celui-ci. En particulier, lorsque l'IA fait des recommandations qui diff√®rent fortement des attentes du clinicien, l'explicabilit√© permet de v√©rifier si les param√®tres pris en compte par le syst√®me sont <strong>pertinents</strong> d'un point de vue clinique.</p><h2 id="techniques-de-l'ia-explicable" tabindex="-1"><a class="header-anchor" href="#techniques-de-l'ia-explicable"></a> Techniques de l'IA explicable</h2><ul><li>Concernant les images m√©dicales, des techniques telles que des <strong>cartes d'attention</strong> ou <strong>cartes de chaleur</strong>, qui mettent en valeur les r√©gions de l'image qui ont le plus influenc√© la d√©cision du mod√®le, sont utilis√©es pour augmenter la confiance des radiologues dans le r√©sultat de l'algorithme. En effet, un ing√©nieur qui travaille dans le domaine m√©dical nous raconte, dans une interview que j'ai mis en annexe, qu'&quot;une carte de chaleur peut √™tre utilis√©e sur les limites de la segmentation pour montrer l'incertitude dans les fronti√®res [...]. Par exemple, pour les d√©tections de tumeurs, cela permet aux cliniciens d'<em>analyser plus de donn√©es</em> provenant des r√©sultats de l'IA et d'avoir un <strong>indice</strong> pour v√©rifier plus en d√©tail o√π la tumeur pourrait se trouver. La variabilit√© des r√©sultats est plus grande.&quot; Ces cartes de chaleur affinent le r√©sultat et permettent peut-√™tre √† l'utilisateur final d'avoir une segmentation plus pr√©cise apr√®s un post-processing de l'image.</li></ul><p><img src="https://www.mdpi.com/applsci/applsci-13-03438/article_deploy/html/images/applsci-13-03438-g007.png" alt="xai dans segmentation de gliome du cerveau"></p><p>La figure ci-dessus inclut l'image originale, les r√©sultats pr√©dits par l'IA (Seg-Pre), la ground-truth (Seg-True) qui est l'image t√©moin consid√©r√©e comme √©tant la v√©rit√©, une carte de chaleur qui repr√©sente les zones o√π s'est focalis√© le mod√®le (CAM++) ainsi que la superposition des segmentations donn√©es par le mod√®le et les cartes de chaleur explicables (C+Pre). Concernant la carte de chaleur, le bleu indique que la pr√©pond√©rence est faible tandis que le rouge indique que la zone a fortement contribu√© au raisonnement du mod√®le.</p><ul><li><p>Pour expliquer le comportement interne d'un r√©seau de neurones avec une description compr√©hensible par l'humain, des <strong>Concept Activator Vectors</strong> peuvent √™tre utilis√©s. [3] <img src="https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fi.ytimg.com%2Fvi%2F-y0oghbEHMM%2Fmaxresdefault.jpg&amp;f=1&amp;nofb=1&amp;ipt=ac68d5955139290c46221769b654cd504989074f3d4d487fc0db1fb56b2e09dd&amp;ipo=images" alt="CAV"> Les Concept Activator Vector d'un concept sont des vecteurs dans la direction des valeurs (par exemple les valeurs d'activation) des exemples du concept [5]. Les auteurs du papier [5] d√©finissent un CAV tel que &quot;la normale d'un hyperplan s√©parant les exemples sans concept et les exemples avec un concept dans l'activation du mod√®le&quot;. C'est un peu complexe mais je cite cet outil pour montrer qu'il existe des fonctions, des concepts, permettant de mod√©liser des mod√®les de Machine Learning complexes pour les rendre interpr√©tables par un humain.</p></li><li><p>Des <strong>librairies open-source</strong> ont √©t√© d√©velopp√©es pour visualiser, d√©bugger, analyser les biais des mod√®les de Machine Learning. En voici quelques noms :</p><ul><li><em>ELI5</em> sur Python pour visualiser et d√©bugger les mod√®les de ML (d√©velopp√© par le MIT);</li><li><em>AI Fairness 360</em> sur Python et R pour analyser et d√©tecter les biais dans les mod√®les d'IA et les datasets (d√©velopp√© par IBM);</li><li><em>What If Tool</em> (WIT) pour visualiser les comportements de mod√®les de ML d√©j√† entra√Æn√©s (d√©velopp√© par Google);</li><li><em>Skater</em> analyse le comportement de mod√®les de ML d√©j√† entra√Æn√©s (d√©velopp√© par Oracle);</li></ul></li><li><p>Dans le cas d'aide au diagnostic, des <strong>arbres de d√©cisions</strong> peuvent expliquer pas-√†-pas comment le mod√®le en arrive au diagnostic en fonction des sympt√¥mes du patient et de ses r√©sultats.</p></li></ul><p><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0045790624002982-gr5_lrg.jpg" alt="xai"></p><p>Une image qui r√©sume les diff√©rentes utilisations possibles de l'IA explicable dans la sant√© [3]</p><h2 id="conclusion" tabindex="-1"><a class="header-anchor" href="#conclusion"></a> Conclusion</h2><p>En conclusion, l'IA explicable dans le contexte m√©dical vise √† rendre les d√©cisions de l'IA plus transparentes et compr√©hensibles, permettant aux cliniciens de v√©rifier et valider les r√©sultats en s'appuyant sur leur expertise clinique. Cela renforce la confiance et aide √† minimiser les risques associ√©s √† l'usage de l'IA en m√©decine. Je vous encourage √† jeter un coup d'oeil aux articles que j'ai list√© dans les sources ainsi qu'aux diff√©rents outils d'explicabilit√© pour avoir une id√©e de quel outil appliquer √† votre mod√®le d'IA.</p><h2 id="annexe-%3A-interview-(vo-en-anglais)-d'un-ing%C3%A9nieur-dans-la-recherche%2C-qui-travaille-sur-des-mod%C3%A8les-de-machine-learning-dans-le-domaine-de-la-cardiologie" tabindex="-1"><a class="header-anchor" href="#annexe-%3A-interview-(vo-en-anglais)-d'un-ing%C3%A9nieur-dans-la-recherche%2C-qui-travaille-sur-des-mod%C3%A8les-de-machine-learning-dans-le-domaine-de-la-cardiologie"></a> Annexe : Interview (VO en anglais) d'un ing√©nieur dans la recherche, qui travaille sur des mod√®les de machine learning dans le domaine de la cardiologie</h2><p>_____ <strong>You are an engineer developping a platform that helps surgeons to plan a cardiac surgery (called Left Atrial Appendage Occlusion). You also have a master co-directed by 5 universities in Barcelona in Computer Vision. Can you tell us a bit more about the use of AI in your work ?</strong></p><p>We do <strong>segmentations</strong> of the left atrial of the heart in order to get a <strong>3D reconstruction</strong> to determine the size of occluder devices to treat atrial fibrillations-derived thrombus with a minimally-invasive intervention (called <em>Left atrial appendage occlusion</em>).</p><p>We use AI for these segmentations of medical images, I did my TFM (<em>note from the interviewer : it's a master thesis in Spain</em>) on that with the pipeline nnU-Net. It's a pipeline containing a pre-trained model for segmentations with pre-processing that reduce the need of hyperparametrisation, it's designed to work well with medical images like CT, MRIs...</p><p>We use another algorithm using Machine Learning to detect the position of the ostium (part of the heart) and the mitral valves on medical images.</p><p>We compute hemo-dynamics simulations manually, without AI. Well, a PhD student tried to predict the 4D MRI flows with AI but we do most of them by hand.</p><p>We also use ChatGpt and Copilot for coding as <strong>assistance-tools</strong> for error solving and making templates for problems that we encounter.</p><p>_____<strong>What type of data do you use to train the model ? How do you collect it ?</strong></p><p>In my case, all the data was coming from the Hospital of Bordeaux. Around 100 cases were previously segmented by colleagues, the input data is DICOM then it's processed with Python librairies to transform it to NIFTI. They are 3D images : it's a series of images that represents a volume. Each one of these images is segmented and with the join of them, you can reconstruct a 3D segmentation or mesh.</p><p>_____<strong>Why do you have to convert the data to Nifti ?</strong></p><p>It's just because the format is easier to handle ! DICOM is a folder with a lot of files while Nifti it's just a file that is compressed. Also, Totalsegmentator (<em>note from the interviewer : it's a tool for segmentations using the nn-Unet method</em>) accepts only Nifti data as input so that's why.</p><p>_____<strong>You work hand-to-hand with clinicians in this academical context. Are they familiar with how the AI models work ? How do you explain to them how it works ? (meetings ?)</strong></p><p>I would say that there are two things : I didn't have to explain [how it works] because a clinician isn't interested in specifically how it works but only in the fact <em>that it's working</em>. In a segmentation, they see the result and check that the segmentation makes sense. In the case of an AI they have the tools to verify it. The AI can <strong>never</strong> take responsability of a medical decision, in any aspect from segmentation to diagnosis so they have to verify it themselves.</p><p>Well, explainable AI isn't mostly to explain how the AI works by itself but more on <strong>which the decisions it takes</strong> and on what things it's based to take the decision. You want to avoid the neural network to be a Black Box.</p><p>In the segmentation, the heatmap can be useful, for example a heatmap can be use on the borders to show the confidence in the borders because there are some irregularities. For exemple, for tumor detections, it allows the clinicians to analyse more data that come from the output of the AI and to have a hint to check in more details where the tumor could be. You have more variability in the output.</p><p>The literal result of the AI is always processed, the output layer usually have the threshold to simplify all the intermediate features to only give to the final user one simplified result.</p><p>_____<strong>For your TFM, you wanted to fine-tune a model, do you have an example of bias and tendencies in the data that could have affected the results ?</strong></p><p>Yes, a lot. First of all, all the patients were <strong>from the same hospital</strong>. All the images came from the same machine while images from different machines could have had slightly different qualities... It would have make the AI potentially <strong>worst</strong> with inputs from other hospital. The contrast and other things used to emphasize the borders of the organ could be different and affect the results of the AI reasoning.</p><p>We had 300 medical images but only 100 of them were segmented. I realized when I analysed them that the ones that were segmented were the ones that were <strong>easier</strong> to segment. The human segmentators chose the easiest ones, that's logical but the AI is going to be worst for the images with more difficulty to segment, for example <strong>low contrasted images</strong>. The clinician can put contrast to detect in a more easy way the organ but if the image was acquired without contrast, the AI isn't used to detect the organ without color or contrast so the results would be bad.</p><p>I have to say that we shouldn't confound <strong>biais</strong> and <strong>intented fine-tuning</strong>. We can fine-tune a model with data of people from a similar demographics for specific reasons but you tend to aim for generalization.</p><p>_____<strong>Thank you so much for your time !</strong></p><h2 id="annexe-%3A-interview-(la-traduction-en-fran%C3%A7ais)-d'un-ing%C3%A9nieur-dans-la-recherche%2C-qui-travaille-sur-des-mod%C3%A8les-de-machine-learning-dans-le-domaine-de-la-cardiologie" tabindex="-1"><a class="header-anchor" href="#annexe-%3A-interview-(la-traduction-en-fran%C3%A7ais)-d'un-ing%C3%A9nieur-dans-la-recherche%2C-qui-travaille-sur-des-mod%C3%A8les-de-machine-learning-dans-le-domaine-de-la-cardiologie"></a> Annexe : Interview (la traduction en fran√ßais) d'un ing√©nieur dans la recherche, qui travaille sur des mod√®les de machine learning dans le domaine de la cardiologie</h2><p>_____ <strong>Vous √™tes ing√©nieur et vous d√©veloppez une plateforme qui aide les chirurgiens √† planifier une op√©ration cardiaque (appel√©e Occlusion de l'appendice auriculaire gauche). Vous √™tes √©galement titulaire d'un master codirig√© par 5 universit√©s de Barcelone en vision par ordinateur. Pouvez-vous nous en dire plus sur l'utilisation de l'IA dans votre travail ?</strong></p><p>Nous effectuons des segmentations de l'oreillette gauche du c≈ìur afin d'obtenir une reconstruction en 3D pour d√©terminer la taille des dispositifs d'occlusion pour traiter les thrombus d√©riv√©s des fibrillations auriculaires par une intervention mini-invasive (appel√©e occlusion de l'appendice auriculaire gauche).</p><p>Nous utilisons l'IA pour ces segmentations d'images m√©dicales, j'ai fait mon TFM (<em>note de l'interviewer : c'est une th√®se de master en Espagne</em>) sur ce sujet avec le pipeline nnU-Net. Il s'agit d'un pipeline contenant un mod√®le pr√©-entra√Æn√© pour les segmentations avec un pr√©-traitement qui r√©duit le besoin d'hyperparam√©trage, il est con√ßu pour fonctionner correctement avec des images m√©dicales comme les CT, les IRM...</p><p>Nous utilisons un autre algorithme utilisant l'apprentissage automatique pour d√©tecter l'ostium (partie du c≈ìur) et les valves mitrales sur les images m√©dicales.</p><p>Nous calculons les simulations h√©modynamiques manuellement, sans IA. Un √©tudiant en doctorat a essay√© de pr√©dire les flux de l'IRM 4D avec l'IA, mais nous faisons la plupart d'entre eux √† la main.</p><p>Nous utilisons √©galement ChatGpt et Copilot pour le codage en tant qu'outils d'assistance pour la r√©solution des erreurs et la cr√©ation de mod√®les pour les probl√®mes que nous rencontrons.</p><p>_____<strong>Quel type de donn√©es utilisez-vous pour entra√Æner le mod√®le ? Comment les recueillez-vous ?</strong></p><p>Dans mon cas, toutes les donn√©es provenaient de l'h√¥pital de Bordeaux. Une centaine de cas ont √©t√© pr√©alablement segment√©s par des coll√®gues, les donn√©es d'entr√©e sont des DICOM puis elles sont trait√©es avec des librairies Python pour les transformer en NIFTI. Il s'agit d'images 3D : c'est une s√©rie d'images qui repr√©sente un volume. Chacune de ces images est segment√©e et la r√©union de ces images permet de reconstruire une segmentation 3D ou un maillage.</p><p>_____<strong>Pourquoi faut-il convertir les donn√©es en Nifti ?</strong></p><p>Tout simplement parce que le format est plus facile √† manipuler ! DICOM est un dossier avec beaucoup de fichiers alors que Nifti est juste un fichier qui est compress√©. De plus, Totalsegmentator (<em>note de l'interviewer : c'est un outil de segmentation utilisant la m√©thode nn-Unet</em>) n'accepte que des donn√©es Nifti en entr√©e, c'est donc pour cela.</p><p>_____<strong>Vous travaillez en √©troite collaboration avec des cliniciens dans ce contexte acad√©mique. Connaissent-ils le fonctionnement des mod√®les d'IA ? Comment leur expliquez-vous ce fonctionnement ? (r√©unions ?)</strong></p><p>Je dirais qu'il y a deux choses : Je n'ai pas eu √† expliquer [comment √ßa marche] parce qu'un clinicien n'est pas int√©ress√© par la fa√ßon dont √ßa marche, mais seulement par le fait que √ßa marche. Dans le cas d'une segmentation, il voit le r√©sultat et v√©rifie que la segmentation a un sens. Dans le cas d'une IA, il dispose des outils n√©cessaires pour la v√©rifier. L'IA ne peut jamais assumer la responsabilit√© d'une d√©cision m√©dicale, quel qu'en soit l'aspect, de la segmentation au diagnostic, et les m√©decins doivent donc la v√©rifier eux-m√™mes.</p><p>En fait, l'IA explicable ne consiste pas principalement √† expliquer comment l'IA fonctionne en elle-m√™me, mais plut√¥t quelles sont les d√©cisions qu'elle prend et sur quels √©l√©ments elle se base pour prendre ces d√©cisions. Il faut √©viter que le r√©seau neuronal ne devienne une bo√Æte noire.</p><p>Dans la segmentation, la carte de chaleur peut √™tre utile, par exemple une carte thermique peut √™tre utilis√©e sur les limites de la segmentation pour montrer l'incertitude dans les fronti√®res car il peut y avoir des irr√©gularit√©s. Par exemple, pour les d√©tections de tumeurs, cela permet aux cliniciens d'analyser plus de donn√©es provenant des r√©sultats de l'IA et d'avoir un indice pour v√©rifier plus en d√©tail o√π la tumeur pourrait se trouver. La variabilit√© des r√©sultats est plus grande.</p><p>Le r√©sultat litt√©ral de l'IA est toujours trait√©, la couche de sortie dispose g√©n√©ralement d'un seuil permettant de simplifier toutes les caract√©ristiques interm√©diaires pour ne donner √† l'utilisateur final qu'un r√©sultat simplifi√©.</p><p>_____<strong>Pour votre TFM, vous vouliez affiner un mod√®le, avez-vous un exemple de biais et de tendances dans les donn√©es qui auraient pu affecter les r√©sultats ?</strong></p><p>Oui, beaucoup. Tout d'abord, tous les patients provenaient du m√™me h√¥pital. Toutes les images provenaient de la m√™me machine, alors que les images provenant de machines diff√©rentes auraient pu avoir des qualit√©s l√©g√®rement diff√©rentes... L'IA aurait √©t√© potentiellement moins bonne avec des images provenant d'autres h√¥pitaux. Le contraste et d'autres √©l√©ments utilis√©s pour mettre en √©vidence les contours de l'organe pouvaient √™tre diff√©rents et affecter les r√©sultats du raisonnement de l'IA.</p><p>Nous avions 300 images m√©dicales, mais seules 100 d'entre elles ont √©t√© segment√©es. En les analysant, je me suis rendu compte que les images segment√©es √©taient celles qui √©taient les plus faciles √† segmenter. Les segmentateurs humains ont choisi les plus faciles, c'est logique, mais l'IA sera moins performante pour les images plus difficiles √† segmenter, par exemple les images faiblement contrast√©es. Le clinicien peut mettre du contraste pour d√©tecter plus facilement l'organe, mais si l'image a √©t√© acquise sans contraste, l'IA n'est pas habitu√©e √† d√©tecter l'organe sans couleur ni contraste, et les r√©sultats seront donc mauvais.</p><p>Je dois dire qu'il ne faut pas confondre le biais et le r√©glage fin intentionnel. Nous pouvons affiner un mod√®le avec des donn√©es de personnes issues d'un groupe d√©mographique similaire pour des raisons sp√©cifiques, mais on a tendance √† viser la g√©n√©ralisation.</p><p>_____<strong>Merci pour le temps que tu m'as accord√© !</strong></p></article></main><footer class="min-h-[50px] border-t-2 mt-4 border-gray-200 dark:border-neutral-700"><div class="max-w-[1000px] mx-auto px-4"><div class="min-h-[50px] flex justify-center items-center"><p class="text-center">¬©2025 <b><span style="font-family:Consolas,sans-serif">Do-<span style="color:#4a86e8">It</span></span></b> - D√©veloppement, Management et Gestion de projets en IT</p></div></div></footer><script src="https://cdn.jsdelivr.net/npm/mathjax/es5/tex-svg-full.js" defer="">window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]]},svg:{fontCache:"global"}},document.addEventListener("DOMContentLoaded",(()=>{MathJax.typeset()}))</script><script src="https://cdn.jsdelivr.net/npm/prismjs/prism.min.js"></script><script src="https://cdn.jsdelivr.net/npm/prismjs/plugins/toolbar/prism-toolbar.min.js"></script><script src="https://cdn.jsdelivr.net/npm/prismjs/plugins/line-numbers/prism-line-numbers.min.js"></script><script src="https://cdn.jsdelivr.net/npm/prismjs/plugins/copy-to-clipboard/prism-copy-to-clipboard.min.js"></script><script src="https://cdn.jsdelivr.net/npm/prismjs/plugins/normalize-whitespace/prism-normalize-whitespace.min.js">Prism.plugins.NormalizeWhitespace.setDefaults({"remove-trailing":!0,"remove-indent":!0,"left-trim":!0,"right-trim":!0})</script><script src="https://cdn.jsdelivr.net/npm/prismjs/plugins/show-language/prism-show-language.min.js"></script><script src="https://cdn.jsdelivr.net/npm/prismjs/plugins/autoloader/prism-autoloader.min.js"></script></body></html>